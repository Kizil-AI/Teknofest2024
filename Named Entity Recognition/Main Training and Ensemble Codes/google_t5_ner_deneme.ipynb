{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s5qlqI-etnc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[torch] datasets tokenizers seqeval bitsandbytes -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGFDaV90_AJ5"
      },
      "source": [
        "# Token classification\n",
        "\n",
        "The first application we‚Äôll explore is token classification. This generic task encompasses any problem that can be formulated as ‚Äúattributing a label to each token in a sentence,‚Äù such as:\n",
        "\n",
        "**Named entity recognition (NER):** Find the entities (such as persons, locations, or organizations) in a sentence. This can be formulated as attributing a label to each token by having one class per entity and one class for ‚Äúno entity.‚Äù\n",
        "\n",
        "**Part-of-speech tagging (POS):** Mark each word in a sentence as corresponding to a particular part of speech (such as noun, verb, adjective, etc.).\n",
        "\n",
        "**Chunking:** Find the tokens that belong to the same entity. This task (which can be combined with POS or NER) can be formulated as attributing one label (usually B-) to any tokens that are at the beginning of a chunk, another label (usually I-) to tokens that are inside a chunk, and a third label (usually O) to tokens that don‚Äôt belong to any chunk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT4r9bTC_AJ7"
      },
      "source": [
        "* O means the word doesn‚Äôt correspond to any entity.\n",
        "* B-PER/I-PER means the word corresponds to the beginning of/is inside a person entity.\n",
        "* B-ORG/I-ORG means the word corresponds to the beginning of/is inside an organization entity.\n",
        "* B-LOC/I-LOC means the word corresponds to the beginning of/is inside a location entity.\n",
        "* B-MISC/I-MISC means the word corresponds to the beginning of/is inside a miscellaneous entity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLQdJuDF_AJ8"
      },
      "source": [
        "### https://huggingface.co/course/chapter7/2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KooHZMYXeymA"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "from transformers import AutoModelForTokenClassification\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import json\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll2003 = datasets.load_dataset(\"conll2003\",trust_remote_code=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyoNAYlEXWRr",
        "outputId": "fddcd825-d3a8-46eb-d0fe-36607a8172e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9A7Jy37_AJ9",
        "outputId": "03842d14-999d-49c9-a1e0-510ed506e60a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 14041\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3250\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
              "        num_rows: 3453\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "conll2003"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSaCXJh7_AJ-",
        "outputId": "8fbeb739-c2c4-4c1b-9a40-597e421c1999"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "conll2003.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXAzwpSNe4FM",
        "outputId": "e7f8f042-9fa1-491a-d94f-8ca50b7effcc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '0',\n",
              " 'tokens': ['EU',\n",
              "  'rejects',\n",
              "  'German',\n",
              "  'call',\n",
              "  'to',\n",
              "  'boycott',\n",
              "  'British',\n",
              "  'lamb',\n",
              "  '.'],\n",
              " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
              " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
              " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "conll2003[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6w_AVpbe6BZ",
        "outputId": "bc2331db-a721-4b29-ea90-dff60eb82ec1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequence(feature=ClassLabel(names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "conll2003[\"train\"].features[\"ner_tags\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "Q1fFfrCY_AJ_",
        "outputId": "88aff16f-d55c-4713-8597-aeb1e5ba3396"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on\\nfour types of named entities: persons, locations, organizations and names of miscellaneous entities that do\\nnot belong to the previous three groups.\\n\\nThe CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on\\na separate line and there is an empty line after each sentence. The first item on each line is a word, the second\\na part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags\\nand the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only\\nif two phrases of the same type immediately follow each other, the first word of the second phrase will have tag\\nB-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase. Note the dataset uses IOB2\\ntagging scheme, whereas the original dataset uses IOB1.\\n\\nFor more details see https://www.clips.uantwerpen.be/conll2003/ner/ and https://www.aclweb.org/anthology/W03-0419\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "conll2003['train'].description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPdTN25De8qy",
        "outputId": "088bfae2-1b58-4cd8-b0ab-534ddb33f2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/t5-base-conll03-english\")\n",
        "#tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWOeOxlP_AKA"
      },
      "source": [
        "# Problem of consecutive subwords.\n",
        "\n",
        "### Note that transformers are often pretrained with subword tokenizers, meaning that even if your inputs have been split into words already, each of those words could be split again by the tokenizer.\n",
        "\n",
        "### This means that we need to do some processing on our labels as the input ids returned by the tokenizer are longer than the lists of labels our dataset contain.\n",
        "\n",
        "This is happening, first because some special tokens might be added (we can a [CLS] and a [SEP] above) and then because of those possible splits of words in multiple tokens:\n",
        "\n",
        "## Strategy to handle above - Here we set the labels of all special tokens to -100 (the index that is ignored by PyTorch) and the labels of all other tokens to the label of the word they come from. Another strategy is to set the label only on the first token obtained from a given word, and give a label of -100 to the other subtokens from the same word. We propose the two strategies here, just change the value of the following flag:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-W_ZVTG_AKB"
      },
      "source": [
        "-----------------------------------\n",
        "\n",
        "### Setting ‚Äì100 as the label for these special tokens and the subwords we wish to mask during training:\n",
        "\n",
        "Why did we choose ‚Äì100 as the ID to mask subword representations? The reason is\n",
        "that in PyTorch the cross-entropy loss class torch.nn.CrossEntropyLoss has an\n",
        "attribute called ignore_index whose value is ‚Äì100. This index is ignored during\n",
        "training,\n",
        "\n",
        "Also we can use it to ignore the tokens associated with consecutive subwords.\n",
        "\n",
        "-----------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uasGvMFo_AKB"
      },
      "source": [
        "## Below cell are just for checking the output of some variables before applying `tokenize_and_align_labels()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDVOKocl_AKC",
        "outputId": "bc997ac3-4a38-4ca6-b7ab-89ac10fa2575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 1, 2, 3, 4, 5, 6, 7, 8, 8, None]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [3371, 15092, 7, 2968, 580, 12, 30242, 2390, 17871, 3, 5, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "example_text = conll2003['train'][0]\n",
        "\n",
        "tokenized_input = tokenizer(example_text[\"tokens\"], is_split_into_words=True)\n",
        "\n",
        "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "\n",
        "word_ids = tokenized_input.word_ids()\n",
        "\n",
        "print(word_ids)\n",
        "\n",
        "''' As we can see, it returns a list with the same number of elements as our processed input ids, mapping special tokens to None and all other tokens to their respective word. This way, we can align the labels with the processed input ids. '''\n",
        "\n",
        "tokenized_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA-KLQ-D_AKC"
      },
      "source": [
        "## Problem of Sub-Token - The  input ids returned by the tokenizer are longer than the lists of labels our dataset contain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBU3vnEMTa4M",
        "outputId": "bb1e4125-1d29-48ee-cda4-84ef7c3a4170"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 0, 7, 0, 0, 0, 7, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "example_text[\"ner_tags\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOb4PpsnTa4M",
        "outputId": "6dfc8752-d9a2-4de5-eecd-77fedcec643d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['‚ñÅEU',\n",
              " '‚ñÅreject',\n",
              " 's',\n",
              " '‚ñÅGerman',\n",
              " '‚ñÅcall',\n",
              " '‚ñÅto',\n",
              " '‚ñÅboycott',\n",
              " '‚ñÅBritish',\n",
              " '‚ñÅlamb',\n",
              " '‚ñÅ',\n",
              " '.',\n",
              " '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhQzTgKg_AKC",
        "outputId": "cc500641-543e-4514-bbf3-fff58f3c6e2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 12)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "len(example_text['ner_tags']), len(tokenized_input[\"input_ids\"])\n",
        "# (9, 11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsMWHr9r_AKD"
      },
      "source": [
        "## The below function `tokenize_and_align_labels` does 2 jobs\n",
        "\n",
        "1. set ‚Äì100 as the label for these special tokens and the subwords we wish to mask during training\n",
        "2. mask the subword representations after the first subword\n",
        "\n",
        "\n",
        "### Then we align the labels with the token ids using the strategy we picked:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FX3JmnRfAv7"
      },
      "outputs": [],
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens=True):\n",
        "    \"\"\"\n",
        "    Function to tokenize and align labels with respect to the tokens. This function is specifically designed for\n",
        "    Named Entity Recognition (NER) tasks where alignment of the labels is necessary after tokenization.\n",
        "\n",
        "    Parameters:\n",
        "    examples (dict): A dictionary containing the tokens and the corresponding NER tags.\n",
        "                     - \"tokens\": list of words in a sentence.\n",
        "                     - \"ner_tags\": list of corresponding entity tags for each word.\n",
        "\n",
        "    label_all_tokens (bool): A flag to indicate whether all tokens should have labels.\n",
        "                             If False, only the first token of a word will have a label,\n",
        "                             the other tokens (subwords) corresponding to the same word will be assigned -100.\n",
        "\n",
        "    Returns:\n",
        "    tokenized_inputs (dict): A dictionary containing the tokenized inputs and the corresponding labels aligned with the tokens.\n",
        "    \"\"\"\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        # word_ids() => Return a list mapping the tokens\n",
        "        # to their actual word in the initial sentence.\n",
        "        # It Returns a list indicating the word corresponding to each token.\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        # Special tokens like `<s>` and `<\\s>` are originally mapped to None\n",
        "        # We need to set the label to -100 so they are automatically ignored in the loss function.\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # set ‚Äì100 as the label for these special tokens\n",
        "                label_ids.append(-100)\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # if current word_idx is != prev then its the most regular case\n",
        "                # and add the corresponding token\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                # to take care of sub-words which have the same word_idx\n",
        "                # set -100 as well for them, but only if label_all_tokens == False\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "                # mask the subword representations after the first subword\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssUnW4fyfDTc",
        "outputId": "df963384-3c34-4599-a833-52bd57cf5c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[3434, 3, 31, 7, 6978, 12, 8, 1611, 3545, 3, 31, 7, 3, 22987, 4492, 4444, 687, 1027, 3108, 2434, 243, 30, 2875, 3674, 225, 805, 15184, 51, 1544, 45, 1440, 119, 145, 7190, 552, 8, 4290, 1867, 47, 964, 49, 3, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[5, 0, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]]}\n"
          ]
        }
      ],
      "source": [
        "q = tokenize_and_align_labels(conll2003['train'][4:5])\n",
        "print(q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A08S1NJn_AKD"
      },
      "source": [
        "### So before applying the `tokenize_and_align_labels()` the `tokenized_input` has 3 keys\n",
        "- input_ids\n",
        "- token_type_ids\n",
        "- attention_mask\n",
        "\n",
        "But after applying `tokenize_and_align_labels()` we have an extra key - `'labels'`\n",
        "\n",
        "\n",
        "==================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyw5ULrtfFfC",
        "outputId": "e6601ab7-406d-4231-d3e0-e5443e0ca1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ñÅGermany________________________________ 5\n",
            "‚ñÅ_______________________________________ 0\n",
            "'_______________________________________ 0\n",
            "s_______________________________________ 0\n",
            "‚ñÅrepresentative_________________________ 0\n",
            "‚ñÅto_____________________________________ 0\n",
            "‚ñÅthe____________________________________ 0\n",
            "‚ñÅEuropean_______________________________ 3\n",
            "‚ñÅUnion__________________________________ 4\n",
            "‚ñÅ_______________________________________ 0\n",
            "'_______________________________________ 0\n",
            "s_______________________________________ 0\n",
            "‚ñÅ_______________________________________ 0\n",
            "veterinary______________________________ 0\n",
            "‚ñÅcommittee______________________________ 0\n",
            "‚ñÅWer____________________________________ 1\n",
            "ner_____________________________________ 1\n",
            "‚ñÅZ______________________________________ 2\n",
            "wing____________________________________ 2\n",
            "mann____________________________________ 2\n",
            "‚ñÅsaid___________________________________ 0\n",
            "‚ñÅon_____________________________________ 0\n",
            "‚ñÅWednesday______________________________ 0\n",
            "‚ñÅconsumers______________________________ 0\n",
            "‚ñÅshould_________________________________ 0\n",
            "‚ñÅbuy____________________________________ 0\n",
            "‚ñÅsheep__________________________________ 0\n",
            "m_______________________________________ 0\n",
            "eat_____________________________________ 0\n",
            "‚ñÅfrom___________________________________ 0\n",
            "‚ñÅcountries______________________________ 0\n",
            "‚ñÅother__________________________________ 0\n",
            "‚ñÅthan___________________________________ 0\n",
            "‚ñÅBritain________________________________ 5\n",
            "‚ñÅuntil__________________________________ 0\n",
            "‚ñÅthe____________________________________ 0\n",
            "‚ñÅscientific_____________________________ 0\n",
            "‚ñÅadvice_________________________________ 0\n",
            "‚ñÅwas____________________________________ 0\n",
            "‚ñÅclear__________________________________ 0\n",
            "er______________________________________ 0\n",
            "‚ñÅ_______________________________________ 0\n",
            "._______________________________________ 0\n",
            "</s>____________________________________ -100\n"
          ]
        }
      ],
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(q[\"input_ids\"][0]),q[\"labels\"][0]):\n",
        "    print(f\"{token:_<40} {label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg0c2D7yfbf0"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = conll2003.map(tokenize_and_align_labels, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "compute_dtype = getattr(torch,\"float16\")\n",
        "\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        ")"
      ],
      "metadata": {
        "id": "djGciwZUOfeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kSF7QRDQffPB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel\n",
        "#model = AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=9)\n",
        "model = AutoModel.from_pretrained(\"dbmdz/t5-base-conll03-english\",quantization_config=quant_config, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okyF8VWWfhnv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1d49ad7-21f9-4b38-bece-e5602226bd20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "args = TrainingArguments(\n",
        "\"test-ner\",\n",
        "evaluation_strategy = \"epoch\",\n",
        "learning_rate=2e-5,\n",
        "per_device_train_batch_size=2,\n",
        "per_device_eval_batch_size=2,\n",
        "num_train_epochs=3,\n",
        "weight_decay=0.01,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3fNcCdRfkV0"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG9bdKiifovx",
        "outputId": "47edac12-8afe-4123-f400-1aae9c349b46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-66cc329c58b9>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric(\"seqeval\",trust_remote_code=True)\n"
          ]
        }
      ],
      "source": [
        "metric = datasets.load_metric(\"seqeval\",trust_remote_code=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-tpGx0sft-q"
      },
      "outputs": [],
      "source": [
        "example = conll2003['train'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vom5Yo3LfqoN",
        "outputId": "79eb3d98-2a7a-429d-d73d-98bf06a870b6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "label_list = conll2003[\"train\"].features[\"ner_tags\"].feature.names\n",
        "\n",
        "label_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R11EMwO2_AKG",
        "outputId": "d547c31c-ed9e-4fef-cf17-18006d6e1382"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MISC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
              " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
              " 'overall_precision': 1.0,\n",
              " 'overall_recall': 1.0,\n",
              " 'overall_f1': 1.0,\n",
              " 'overall_accuracy': 1.0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "\n",
        "\n",
        "labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
        "\n",
        "metric.compute(predictions=[labels], references=[labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Px8K0rm_AKG"
      },
      "source": [
        "## seqeval - The way the package works by accepting list of lists\n",
        "\n",
        "The seqeval package expects the predictions and labels as lists of lists, with\n",
        "each list corresponding to a single example in our validation or test sets. To\n",
        "integrate these metrics during training, we need a function that can take the\n",
        "outputs of the model and convert them into the lists that seqeval expects.\n",
        "\n",
        "The following does the trick by ensuring we ignore the label IDs associated with\n",
        "subsequent subwords:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IZiTIn3_AKG"
      },
      "source": [
        "## Compute Metrics\n",
        "\n",
        "This compute_metrics() function first takes the argmax of the logits to convert them to predictions (as usual, the logits and the probabilities are in the same order, so we don‚Äôt need to apply the softmax). Then we have to convert both labels and predictions from integers to strings. We remove all the values where the label is -100, then pass the results to the metric.compute() method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tySgh2eY_AKG"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    \"\"\"\n",
        "    Function to compute the evaluation metrics for Named Entity Recognition (NER) tasks.\n",
        "    The function computes precision, recall, F1 score and accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    eval_preds (tuple): A tuple containing the predicted logits and the true labels.\n",
        "\n",
        "    Returns:\n",
        "    A dictionary containing the precision, recall, F1 score and accuracy.\n",
        "    \"\"\"\n",
        "    pred_logits, labels = eval_preds\n",
        "\n",
        "    pred_logits = np.argmax(pred_logits, axis=2)\n",
        "    # the logits and the probabilities are in the same order,\n",
        "    # so we don‚Äôt need to apply the softmax\n",
        "\n",
        "    # We remove all the values where the label is -100\n",
        "    predictions = [\n",
        "        [label_list[eval_preds] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(pred_logits, labels)\n",
        "    ]\n",
        "\n",
        "    true_labels = [\n",
        "      [label_list[l] for (eval_preds, l) in zip(prediction, label) if l != -100]\n",
        "       for prediction, label in zip(pred_logits, labels)\n",
        "   ]\n",
        "    results = metric.compute(predictions=predictions, references=true_labels)\n",
        "    return {\n",
        "   \"precision\": results[\"overall_precision\"],\n",
        "   \"recall\": results[\"overall_recall\"],\n",
        "   \"f1\": results[\"overall_f1\"],\n",
        "  \"accuracy\": results[\"overall_accuracy\"],\n",
        "  }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZieP-5MU_AKG"
      },
      "source": [
        "### `predictions` will print a long 2d tensor like below\n",
        "\n",
        "```\n",
        "[['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['B-LOC', 'O', 'O', 'O', 'O', 'O'], ['B-MISC', 'I-MISC', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', ['O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'B-ORG', 'O', 'B-ORG', 'I-ORG', 'O', 'B-ORG', 'B-ORG', 'B-ORG', 'I-ORG', 'O', 'B-ORG', 'I-ORG', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        ", ['O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNZJvlI3f3dr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "7a91b9e8-dc21-4791-a87a-091740b4c134"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-05062cdad981>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m trainer = Trainer(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenized_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;31m# At this stage the model is already loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_quantized_and_base_model\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_peft_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0;34m\"You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;34m\" the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "   train_dataset=tokenized_datasets[\"train\"],\n",
        "   eval_dataset=tokenized_datasets[\"validation\"],\n",
        "   data_collator=data_collator,\n",
        "   tokenizer=tokenizer,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "4LbUh9shgD5X",
        "outputId": "8312cfe8-5840-4700-ae5e-becd7dd74472"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='751' max='21063' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  751/21063 01:46 < 48:10, 7.03 it/s, Epoch 0.11/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1885\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1886\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2216\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2218\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3237\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3238\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3264\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3265\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3266\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlm_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGmSRYR6L3Ok"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"ner_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX9FrmzaR2XW"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(\"tokenizer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWj16dJeMj10"
      },
      "outputs": [],
      "source": [
        "id2label = {\n",
        "    str(i): label for i,label in enumerate(label_list)\n",
        "}\n",
        "label2id = {\n",
        "    label: str(i) for i,label in enumerate(label_list)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5EZxnPxMjSb"
      },
      "outputs": [],
      "source": [
        "config = json.load(open(\"ner_model/config.json\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_p_ohBAMyQ7"
      },
      "outputs": [],
      "source": [
        "config[\"id2label\"] = id2label\n",
        "config[\"label2id\"] = label2id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBagjGQfM3xo"
      },
      "outputs": [],
      "source": [
        "json.dump(config, open(\"ner_model/config.json\",\"w\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0kLZ-CYWMSNm"
      },
      "outputs": [],
      "source": [
        "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"ner_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QH9hlOroHKyN"
      },
      "outputs": [],
      "source": [
        "nlp = pipeline(\"ner\", model=model_fine_tuned, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "example = \"Michael jordan is one of the best NBA players.\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "\n",
        "print(ner_results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "36cf16204b8548560b1c020c4e8fb5b57f0e4c58016f52f2d4be01e192833930"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}