{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dbmdz/convbert-base-turkish-cased: {'precision': 0.8841978555268938, 'recall': 0.8837137655790506, 'f1': 0.8831126880089364, 'accuracy': 0.8837137655790506} 'eval_runtime': 2.2643\n",
    "\n",
    "savasy/bert-base-turkish-ner-cased: {'precision': 0.8838903284646512, 'recall': 0.8810810810810811, 'f1': 0.8797721987329598, 'accuracy': 0.8810810810810811} 'eval_runtime': 2.1016\n",
    "\n",
    "dbmdz/convbert-base-turkish-mc4-cased: {'precision': 0.8826339221428284, 'recall': 0.8753956028567428, 'f1': 0.8733315810408983, 'accuracy': 0.8753956028567428} 'eval_runtime': 2.2812\n",
    "\n",
    "dbmdz/bert-base-turkish-cased: {'precision': 0.8880127932156088, 'recall': 0.885674275311581, 'f1': 0.8847302272801759, 'accuracy': 0.885674275311581} 'eval_runtime': 2.0843\n",
    "\n",
    "dbmdz/distilbert-base-turkish-cased: {'precision': 0.8840328817567008, 'recall': 0.8821453577930262, 'f1': 0.881240008651113, 'accuracy': 0.8821453577930262} 'eval_runtime': 1.416\n",
    "\n",
    "Buseak/penn_berturk_0203_v5: {'precision': 0.8892508284752364, 'recall': 0.8877748214535779, 'f1': 0.886908411423112, 'accuracy': 0.8877748214535779} 'eval_runtime': 2.0726\n",
    "\n",
    "Buseak/model_from_berturk_Feb_5_TrainTestSplit: {'precision': 0.8853321255544964, 'recall': 0.882313401484386, 'f1': 0.881050226510466, 'accuracy': 0.882313401484386} 'eval_runtime': 2.0887\n",
    "\n",
    "alierenak/berturk_cased_ner: {'precision': 0.8857173940725485, 'recall': 0.8835737291695841, 'f1': 0.882624619740858, 'accuracy': 0.8835737291695841} 'eval_runtime': 2.1025\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google-t5/t5-base: {'precision': 0.8880964309996527, 'recall': 0.8878481804216909, 'f1': 0.8872760039385544, 'accuracy': 0.8878481804216909} 'eval_runtime': 5.2172 sn\n",
    "\n",
    "google-t5/t5-large: {'precision': 0.8861702641954879, 'recall': 0.8855099849104825, 'f1': 0.8848582084053055, 'accuracy': 0.8855099849104825} \n",
    "'eval_runtime':  13.4809 sn\n",
    "\n",
    "ensemble: coef_list =[0.46,0.54]  {'precision': 0.8913930899320432, 'recall': 0.8909748372099346, 'f1': 0.8903341897480656, 'accuracy': 0.8909748372099346} 20sn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ENSEMBLE\n",
    "\n",
    "coef_list = [0.2346441490550527, 0.0,0.1716283241495124, 0.014589917908531563, 0.14050873713031567, 0.24650854469047823, 0.15093320130492155, 0.04118712576118782]\n",
    "\n",
    "{'precision': 0.8967488718473344, 'recall': 0.8943845399803949, 'f1': 0.8934500554617636, 'accuracy': 0.8943845399803949}\n",
    "\n",
    "runtime = 16 sn fln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardcoded Ensemble\n",
    "\n",
    "coef_list = [0.31696459718189396, 0.17699905307486272, -0.13317544755074107, 0.14562928980683465, 0.22494285291506785, 0.014138373741593978, 0.0006222082553536673, 0.08237834873207166]\n",
    "\n",
    "{'precision': 0.8963712859742162, 'recall': 0.8946646127993279, 'f1': 0.893828880708265, 'accuracy': 0.8946646127993279}\n",
    "\n",
    "runtime = 16sn fln"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
